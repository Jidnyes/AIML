# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import kagglehub
from kagglehub import KaggleDatasetAdapter

# Set the path to the file you'd like to load
file_path = "Titanic-Dataset.csv"

# Load the latest version
hf_dataset = kagglehub.load_dataset(
  KaggleDatasetAdapter.HUGGING_FACE,
  "yasserh/titanic-dataset",
  file_path,
  # Provide any additional arguments like
  # sql_query, hf_kwargs, or pandas_kwargs. See
  # the documenation for more information:
  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterhugging_face
)

print("Hugging Face Dataset:", hf_dataset)


# Convert Hugging Face dataset to pandas DataFrame for easier manipulation
df = pd.DataFrame(hf_dataset)

# Display initial information about the dataset
print("\nInitial Dataset Info:")
df.info()

print("\nMissing values before cleaning:")
print(df.isnull().sum())

# Separate target variable
X = df.drop('Survived', axis=1)
y = df['Survived']

# Define features to be transformed
numerical_features = ['Age', 'Fare']
categorical_features = ['Pclass', 'Sex', 'Embarked']

# Create preprocessing pipelines for numerical and categorical features
# Impute missing numerical values with the mean
numerical_transformer = SimpleImputer(strategy='mean')

# Impute missing categorical values with the most frequent value and one-hot encode
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Create a column transformer to apply different transformations to different columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Keep other columns (like Name, Ticket, Cabin) as they are
)

# Create the full preprocessing and modeling pipeline (we only do preprocessing here)
pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

# Apply the preprocessing pipeline to the features
X_processed = pipeline.fit_transform(X)

# Convert the processed features back to a DataFrame (optional, but good for inspection)
# Get feature names after one-hot encoding
# Use get_feature_names_out instead of get_feature_names_in
ohe_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)
processed_feature_names = numerical_features + list(ohe_feature_names) + list(X.columns.difference(numerical_features + categorical_features))

# Create processed DataFrame
X_processed_df = pd.DataFrame(X_processed, columns=processed_feature_names)

print("\nProcessed data information:")
X_processed_df.info()

print("\nMissing values after cleaning:")
print(X_processed_df.isnull().sum())

# Example: Display the first few rows of the processed data
print("\nFirst 5 rows of processed data:")
print(X_processed_df.head())

# Split data into training and testing sets (optional, but standard practice)
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

print(f"\nTraining data shape: {X_train.shape}")
print(f"Testing data shape: {X_test.shape}")
print(f"Training target shape: {y_train.shape}")
print(f"Testing target shape: {y_test.shape}")
